# -*- coding: utf-8 -*-
"""VLM(Vision Language Models).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tLZCKqh5CKsyCLAzLHMsP1n_MDSTzsNX

# pip install -U -q byaldi pdf2image qwen-vl-utils transformers
# Tested with byaldi==0.0.4, pdf2image==1.17.0, qwen-vl-utils==0.0.8, transformers==4.45.0

# sudo apt-get install -y poppler-utils

import requests
import os

pdfs = {
    "MALM": "https://www.ikea.com/us/en/assembly_instructions/malm-4-drawer-chest-white__AA-2398381-2-100.pdf",
    "BILLY": "https://www.ikea.com/us/en/assembly_instructions/billy-bookcase-white__AA-1844854-6-2.pdf",
    "BOAXEL": "https://www.ikea.com/us/en/assembly_instructions/boaxel-wall-upright-white__AA-2341341-2-100.pdf",
    "ADILS": "https://www.ikea.com/us/en/assembly_instructions/adils-leg-white__AA-844478-6-2.pdf",
    "MICKE": "https://www.ikea.com/us/en/assembly_instructions/micke-desk-white__AA-476626-10-100.pdf"
}

output_dir = "data"
os.makedirs(output_dir, exist_ok=True)

for name, url in pdfs.items():
    response = requests.get(url)
    pdf_path = os.path.join(output_dir, f"{name}.pdf")

    with open(pdf_path, "wb") as f:
        f.write(response.content)

    print(f"Downloaded {name} to {pdf_path}")

print("Downloaded files:", os.listdir(output_dir))

import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 8, figsize=(15, 10))

for i, ax in enumerate(axes.flat):
    img = all_images[len(all_images)-1][i]
    ax.imshow(img)
    ax.axis('off')

plt.tight_layout()
plt.show()
"""

import os
from pdf2image import convert_from_path

def convert_pdfs_to_images(pdf_folder):
    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]
    all_images = {}

    for doc_id, pdf_file in enumerate(pdf_files):
        pdf_path = os.path.join(pdf_folder, pdf_file)
        images = convert_from_path(pdf_path)
        all_images[doc_id] = images

    return all_images

all_images = convert_pdfs_to_images("./data/")

from byaldi import RAGMultiModalModel

from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor
from qwen_vl_utils import process_vision_info
import torch

import os
# 1) make sure CUDA is ignored globally on this run
os.environ["CUDA_VISIBLE_DEVICES"] = ""
os.environ["TRANSFORMERS_NO_CUDA"] = "1"   # transformers should never pick CUDA

use_mps = torch.backends.mps.is_available()
device = torch.device("mps" if use_mps else "cpu")
dtype  = torch.float16 if use_mps else torch.float32  # MPS prefers float16

docs_retrieval_model = RAGMultiModalModel.from_pretrained(
            "vidore/colpali-v1.2",
            device=device
        )

docs_retrieval_model.index(
    input_path="data/",
    index_name="image_index",
    store_collection_with_index=False,
    overwrite=True
)

"""# Testing the Document Retrieval"""

text_query = "How many people are present in the reshape lab?"

results = docs_retrieval_model.search(text_query, k=3)
results

def get_grouped_images(results, all_images):
    grouped_images = []

    for result in results:
        doc_id = result['doc_id']
        page_num = result['page_num']
        grouped_images.append(all_images[doc_id][page_num - 1]) # page_num are 1-indexed, while doc_ids are 0-indexed. Source https://github.com/AnswerDotAI/byaldi?tab=readme-ov-file#searching

    return grouped_images

grouped_images = get_grouped_images(results, all_images)


print(f"Retrieved {len(grouped_images)} images for the query: '{text_query}'")

# import matplotlib.pyplot as plt

# fig, axes = plt.subplots(1, 3, figsize=(15, 10))

# for i, ax in enumerate(axes.flat):
#     img = grouped_images[i]
#     ax.imshow(img)
#     ax.axis('off')

# plt.tight_layout()
# plt.show()


vl_model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-7B-Instruct",
    torch_dtype=dtype,
)

vl_model = vl_model.to(device).eval()

min_pixels = 224*224
max_pixels = 1024*1024
vl_model_processor = Qwen2VLProcessor.from_pretrained(
    "Qwen/Qwen2-VL-7B-Instruct",
    min_pixels=min_pixels,
    max_pixels=max_pixels
)

# chat_template = [
#     {
#         "role": "user",
#         "content": [
#             {
#                 "type": "image",
#                 "image": grouped_images[0],
#             },
#             {
#                 "type": "image",
#                 "image": grouped_images[1],
#             },
#             {
#                 "type": "image",
#                 "image": grouped_images[2],
#             },
#             {
#                 "type": "text",
#                 "text": text_query
#             },
#         ],
#     }
# ]

# text = vl_model_processor.apply_chat_template(
#     chat_template, tokenize=False, add_generation_prompt=True
# )

# image_inputs, _ = process_vision_info(chat_template)
# inputs = vl_model_processor(
#     text=[text],
#     images=image_inputs,
#     padding=True,
#     return_tensors="pt",
# )
# inputs = inputs.to("cuda")

# generated_ids_trimmed = [
#     out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
# ]
# output_text = vl_model_processor.batch_decode(
#     generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
# )

def answer_with_multimodal_rag(vl_model, docs_retrieval_model, vl_model_processor, grouped_images, text_query, top_k, max_new_tokens):
    results = docs_retrieval_model.search(text_query, k=top_k)
    grouped_images = get_grouped_images(results, all_images)

    chat_template = [
    {
      "role": "user",
      "content": [
          {"type": "image", "image": image} for image in grouped_images
            ] + [
          {"type": "text", "text": text_query}
        ],
      }
    ]

    # Prepare the inputs
    text = vl_model_processor.apply_chat_template(chat_template, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(chat_template)
    inputs = vl_model_processor(
        text=[text],
        images=image_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to(device)

    # Generate text from the vl_model
    generated_ids = vl_model.generate(**inputs, max_new_tokens=max_new_tokens)
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]

    # Decode the generated text
    output_text = vl_model_processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )

    return output_text

output_text = answer_with_multimodal_rag(
    vl_model=vl_model,
    docs_retrieval_model=docs_retrieval_model,
    vl_model_processor=vl_model_processor,
    grouped_images=grouped_images,
    text_query="Who're the bigbosses of the LumberHacks 2025 event?",
    top_k=3,
    max_new_tokens=500
)
print(output_text[0])

